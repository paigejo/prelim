\documentclass[xcolor=svgnames]{beamer}

\usetheme{umbc1} 
%\usecolortheme{default}

\usepackage{../mystyle}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{subfig}
\usepackage{capt-of}
\usepackage{natbib}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=,urlcolor=Blue}

% keep only the style and the slide number in the footer:
\setbeamertemplate{footline}[text line]{%
  \vbox{%
    \insertvrule{2.0pt}{umbc@decorations.fg!50!bg}%
    \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex]{umbc@decorations}%
            \Tiny\hspace*{4.65in}\insertframenumber{} %/ \inserttotalframenumber\hspace*{2ex}\hspace{4mm}
    \end{beamercolorbox}}}


%\beamertemplatenavigationsymbolsempty
%\setbeamertemplate{footline}[frame number]

\title{~\\~\\Geostatistical inference under preferential sampling: Final presentation
\\
{\small By Peter Diggle, Raquel Menezes, and Ting-li Su}
}
\author[John Paige]{John Paige}
\date[\today]{\vspace{.3in} \small \\ \today}
\institute[University of Washington] % (optional, but mostly needed)
{
  \vspace{.1in} \\ Statistics Department\\
{\sc  University of Washington  }
}

\begin{document}
\setlength{\columnsep}{.5cm}


\frame{\titlepage}

%---------------------------------------------------------------------------------------------------------------------------------------

%\begin{frame}
 % \frametitle{Outline}
 % \tableofcontents
%\end{frame}


%----------------------------------------------------------------------------------------------------------------------------------------
%  Section 1
%----------------------------------------------------------------------------------------------------------------------------------------
\section{Introduction}

%\begin{frame}
%  \frametitle{Outline}
%  \tableofcontents[currentsection]
%\end{frame}

%\frame{\tableofcontents}

%\subsection{Introduction to Problem}

%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Divisions of Spatial Statistics}

\begin{columns}

\column{2.5in}
\begin{itemize}
\item \citet{Cressie1991} and \citet{gelfand2010} divide spatial statistics into 3 areas:
\begin{itemize}
\item \alert<1>{discrete data}
\item \alert<2>{continuous data}
\item \alert<3>{point patterns}
\end{itemize}
\end{itemize}

\uncover<4>{
\begin{itemize}
\item \citet{diggle2013} instead gives 2 subdivisions of spatial statistics:
\begin{itemize}
\item continuous data
\item discrete data
\end{itemize}
\item This emphasizes random nature of sampling locations
\end{itemize}
}

\column{2.4in}
\begin{center}
\includegraphics[width=2.2in]<2>{continuousData.pdf}
\includegraphics[width=2.2in]<1>{discreteData.pdf}
\includegraphics[width=2.2in]<3>{PPData.pdf}
\includegraphics[width=2.2in]<4>{GPPPData.pdf}
\\{\footnotesize Data simulated with \texttt{RandomFields R} package \citep{RandomFields}}
\end{center}

\end{columns}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Divisions of Spatial Statistics}

\begin{columns}

\column{2.5in}
\begin{itemize}
\setlength\itemsep{.15in}
\item Classically, data locations are assumed to be fixed constants
\item What happens when the sample locations depend on the measured process itself?
\begin{itemize}
\item This is called \tbf{preferential sampling}
\end{itemize}
\end{itemize}

\column{2.5in}
\begin{center}
\includegraphics[width=2.2in]{GPPPData.pdf}
\end{center}

\end{columns}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Problems Addressed}
\begin{columns}

\column{2.5in}
\begin{itemize}
\setlength\itemsep{.15in}
\item Determining if data is sampled preferentially
\item How preferential sampling affects `naive' inference
\item Effective model for preferential sampling
\item Focus on lead levels in Galicia, Spain and simulated experiments
\end{itemize}

\column{2.5in}
\begin{figure}
\centering
\image{width=2.5in}{sampling_locations.pdf}
\end{figure}

\end{columns}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Classical Model}

$$ Y_i = \mu + S(x_i) + Z_i, $$
$Y_i$: observation at location $x_i$\\
$\mu$: mean\\
$S(\v{x}) \sim \MVN{\v{0}, \Sigma(\v{x})}$: spatially correlated portion of process\\
$Z_i \iid \Norm{0, \tau^2}$: measurement noise\\
\uncover<2->{
$\Rightarrow$ $\v{Y} \sim \MVN{\v{\mu}, \Sigma_0}$, where $\Sigma_0 = \Sigma_0(\v{\theta}) = \Sigma + \tau^2I$}\\~\\
\uncover<3->{
Log likelihood:
$$\mcal{L}(\v{\theta}) = -\frac{1}{2}\log(|\Sigma_0|) - \frac{1}{2}(\v{Y} - \v{\mu})' \Sigma_0^{-1} (\v{Y} - \v{\mu}) - \frac{n}{2}\log(2\pi)$$
}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Classical Model}

$$ Y_i = \mu + S(x_i) + Z_i, $$
$Y_i$: observation at location $x_i$\\
$\mu$: mean\\
$S(\v{x}) \sim \MVN{\v{0}, \Sigma(\v{x})}$: spatially correlated portion of process\\
$Z_i \iid \Norm{0, \tau^2}$: measurement noise
\\~\\
Common assumptions:
\begin{itemize}
\item $x_i$ are sampled independently of true process $\mu + S$
\item Stationarity
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Variograms}

\begin{itemize}
\setlength\itemsep{.05in}
\item \tbf{Stationarity}: under stationarity, $\var{S(x_i) - S(x_j)} = V(x_i - x_j)$
\item \tbf{Isotropy}: under isotropy (and stationarity), $\var{S(x_i) - S(x_j)} = V(|x_i - x_j|)$
\item \tbf{Variograms} define the spatial structure of the covariance in $S$
\item Empirical estimate given data $Y_i$ at location $x_i$ (under stationarity and isotropy):
$$ \wh{V}(d) = \frac{1}{| N(d)|} \sum_{|x_i - x_j| \in N(d)} (Y_i - Y_j)^2 $$
where $N(d)$ is the set of pairs $(x_i,x_j)$ with $|x_i - x_j| \approx d$
\item This estimator assumes non-preferentiality
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Variograms}

Mat\'{e}rn theoretical variogram:
$$ V(d) = \sigma^2(1- \rho(u \ \vert \ \phi, \kappa)) + \tau^2$$
where
$$\rho(u \ \vert \ \phi, \kappa) = \frac{1}{2^{\kappa - 1} \Gamma(\kappa)} (u/\phi)^{\kappa} K_\kappa(u/\phi), $$
is the Mat\'{e}rn correlation function\\~\\
$u$: distance\\
$\phi$: scale\\
$\kappa$: smoothness\\
$\sigma^2$: is the variance of $S$\\
$\tau^2$: measurement variance\\
$K_\kappa(\cdot)$: Bessel function

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Variograms of Log-Lead Data (Classical)}

\image{width=2.22in}{vario97.pdf} \image{width=2in}{vario00.pdf}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Model for Preferential Sampling}

Three assumptions for model:
\begin{enumerate}
\item $S$ is still a stationary, mean zero Gaussian process \\
(so $S(\v{x}) \sim \MVN{\v{0}, \Sigma(\v{x})}$)
\item Conditional on $S$, $\v{X}$ is an inhomogeneous Poisson process wtih random intensity
$$\Lambda(x) = \exp{\alpha + \beta S(x)}$$
\item $Y_i \vert S, \v{X} \iid \Norm{\mu + S(x_i), \tau^2}$
\end{enumerate}
~\\
1 + 2 $\Leftrightarrow$ $X$ is a log Gaussian Cox process (LGCP)

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Log-Gaussian Cox Processes}

\begin{columns}[l]

\column{.05in}
\column{2.45in}
A \tbf{Cox process} is a stochastic point process that for $B, B'$ bounded Borel sets satisfies:
\begin{itemize}
\item $\Lambda(x) \geq 0$ is a random intensity
\item $N(B)~\sim~\Pois{\int_B \Lambda(x) \ dx}$
\item $N(B) \indep N(B')$ when $B \cap B' = \emptyset$
\end{itemize}

\column{2.45in}

\begin{figure}
\centering
\image{width=2.45in}{cox_proc_fig.png}
\caption{An example of a LGCP on unit square where $\beta=2$, and $S$ has Mat\'{e}rn covariance.}
\end{figure}

\column{.05in}

\end{columns}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Testing Affect of Sample Designs: Samplings Schemes}
\begin{center}
\centering
\image{width=1.5in}{unifSchemeHRes.png}
\image{width=1.5in}{clustSchemeHRes.png}
\image{width=1.5in}{prefSchemeHRes.png}
\\{\footnotesize Sampling processes for Mat\'{e}rn covariance, $\beta=2$, and $\alpha=1$.}
\end{center}
Variogram estimation tested under 500 simulations from three sampling designs:
\begin{enumerate}
\item[a] Uniform
\item[b] Preferential ($\beta=2$)
\item[c] Clustered
\end{enumerate}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Classical Variogram Bias Under Preferential Sampling}
\begin{figure}
\centering
\vspace{-.0in}
\image{width=4.0in}{VGbias.pdf}
\vspace{-.0in}
\caption{Variogram bias $\pm$2 standard errors and standard deviations for uniform (solid), clustered (dashed), and preferential (dotted) sampling schemes.}
\end{figure}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Classical Prediction Bias Under Preferential Sampling}
\begin{table}[ht]
\centering
\begin{tabular}{|r|cccc|}
\hline
\emph{Mod.} &  \emph{Param.} &  \multicolumn{3}{c|}{\emph{Confidence intervals}}  \\ \cline{3-5}
& & \emph{Uniform} & \emph{Preferential} & \emph{Clustered} \\
\hline
1 & Bias & (-0.03, 0.04) & (0.96, 1.13) & (-0.07, 0.06) \\ 
1 & RMSE & (0.38, 0.42) & (1.31, 1.52) & (0.69, 0.79) \\ 
2 & Bias & (-0.03, 0.02) & (-0.11, -0.06) & (-0.02, 0.04) \\
2 & RMSE & (0.30, 0.32) & (0.32, 0.34)& (0.30, 0.32) \\
\hline
\end{tabular}
\caption{Classical predictions using 95\% Confidence intervals for the given parameters under the given models and sampling schemes}
\end{table}
Models:
\begin{enumerate}
\item $(\mu=4, \sigma^2=1.5, \phi=.15, \kappa=1, \beta=2)$
\item $(\mu=1.51, \sigma^2=.14, \phi=.31, \kappa=.5, \beta=-2.20, \tau^2=.059)$
\end{enumerate}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Preferential Model Likelihood}
Make gridded approximation of $\v{S} = \{\v{S}_0, \v{S}_1\}$ on lattice $\v{X}^* = (x_1^* \ ... \ x_N^*)'$.
\begin{itemize}
\item $\v{S}_0$ are true values at data locations
\item $\v{S}_1$ are true values at other grid points
\end{itemize}
\bal
L(\vt) &= \int \pi(\v{Y} \vert \v{X}, \v{S}) \pi(\v{X} \vert \v{S}) \pi(\v{S}) \ d\v{S} \\
&= \hdots \\
%&= \int \pi(X \vert S) \pi(Y \vert X, S) \frac{\pi(S \vert Y)}{\pi(S \vert Y)} \pi(S) \ dS \\
%&= \int \pi(X \vert S) \pi(Y \vert S_0) \frac{\pi(S \vert Y)}{\pi(S_0 \vert Y) \pi(S_1 \vert S_0, Y)} \pi(S) \ dS \\
%&= \int \pi(X \vert S) \pi(Y \vert S_0) \frac{\pi(S \vert Y)}{\pi(S_0 \vert Y) \pi(S_1 \vert S_0)} \pi(S_0, S_1) \ dS \\
%&= \int \pi(X \vert S) \pi(Y \vert S_0) \frac{\pi(S \vert Y)}{\pi(S_0 \vert Y)} \pi(S_0) \ dS \\
&= E_{\v{S} \vert \v{Y}} \brack{\pi(\v{X} \vert \v{S}) \frac{\pi(\v{Y} \vert \v{S}_0)}{\pi(\v{S}_0 \vert \v{Y})} \pi(\v{S}_0)} \\
&\approx m^{-1} \sum_{j=1}^m \pi(\v{X} \vert \v{S}_j) \frac{\pi(\v{Y} \vert \v{S}_{0j})}{\pi(\v{S}_{0j} \vert \v{Y})} \pi(\v{S}_{0j})
\eal
where $\v{S}_j$ is the $j$th conditional simulation of $S$ conditioned on $\v{Y}$.
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Preferential Model Likelihood}
Define $C$ as a $n \times N$ matrix with a single 1 in each row and all else 0 s.t. $\v{X} = C \v{X}^*$.
\\~\\
Steps for Monte Carlo Simulation:
\begin{enumerate}
\item Simulate $\v{S} \sim \MVN{\v{0}, \Sigma}$ using Circulant Embedding \citep{woodChan1994}
\item Compute $j$th simulation of $\v{S} \vert \v{Y}$:
$$ \v{S}_j \defn \v{S} + \Sigma C' \Sigma_0^{-1}(\v{Y} - \v{\mu} + \v{Z} - C \v{S}) $$
Where $Z_i \iid \Norm{0, \tau^2}$
\item Calculate $m^{-1} \sum_{j=1}^m \pi(\v{X} \vert \v{S}_j) \frac{\pi(\v{Y} \vert \v{S}_{0j})}{\pi(\v{S}_{0j} \vert \v{Y})} \pi(\v{S}_{0j})$
\end{enumerate}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Preferential Model Likelihood}
\bal
\pi(\v{X} \vert \v{S}_j) &= \paren{\prodi \Lambda(x_i)} \paren{\int \Lambda(x) \ dx}^{-n}\\
\v{Y} \vert \v{S}_{0j} &\sim \MVN{ \v{S}_{0j}, \tau^2 I} \\
\v{S}_{0j} \vert \v{Y}  &\sim \MVN{\Sigma C' \Sigma_0^{-1}(\v{Y} - \v{\mu}), \Sigma - \Sigma C' \Sigma_0^{-1} C \Sigma}  \\
\v{S}_{0j} &\sim \MVN{\v{0}, C \Sigma C'} 
\eal
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Goodness of Fit}
\emph{Reduced second moment measure} (or $K$-function) for defined model is given by:
$$ K(s) = \pi s^2 + 2 \pi \int_0^s (\exp{\beta^2 \sigma^2 \rho(u; \kappa, \phi)} - 1) u \ du $$
$s$: distance\\
$\rho(u ; \phi, \kappa)$: Mat\'{e}rn correlation function\\~\\
$K$-functions commonly used in goodness of fit tests
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Goodness of Fit: Monte Carlo Testing}
For any Monte Carlo test statistic, $T$, where higher $T$ casts doubt on $H_0$, assume:
\begin{itemize}
\item $T_1$ is from data
\item  $T_2, ..., T_{n}$ are simulated under $H_0$
\end{itemize}
Then our $p$-value is the rank of $T_1$ out of $T_1, T_2, ..., T_n$ (\emph{i.e.} if $n=100$ and $T_1$ is largest test statistic, $p=.01$).
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Goodness of Fit: Test Statistic}
$$ T = \int_0^{0.25} \frac{\paren{\h{K}(s) - K(s)}^2}{v(s)} \ ds $$
\\~\\
$K(s)$: Theoretical $K$-function under MLEs\\
$\h{K}(s)$: Empirical $K$-function of simulation (or data)\\
$v(s)$: Variance of $\h{K}(s)$
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Goodness of Fit: Monte Carlo Testing}
\begin{figure}
\centering
\image{width=4in}{envelope.pdf}
\end{figure}
$p=0.07$
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Likelihood Fitting}
\begin{itemize}
\item Use Nelder-Mead \citep{nelderMead} simplex algorithm for optimization with naive initial guesses
\begin{itemize}
\item To simulate the $\v{S}_j$'s quickly, I use cutoff embedding \citep{gneiting2012}
\end{itemize}
\item Standard errors and correlations for MLEs inferred using quadratic fit of likelihood surface
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Likelihood Fit: Non-Preferential}
\begin{table}
\centering
\begin{tabular}{ccccccccc}
\hline
&  \emph{Estimate} & \emph{SE} & & \multicolumn{5}{c}{\emph{Correlation matrix}}  \\ 
\hline
$\mu_{97}$ & 1.55 & 0.018 &  & 1 & 0.00 & 0.02 & -0.00 & -0.02 \\ 
$\mu_{00}$ & 0.73 & 0.014 & &  & 1 & 0.10 & 0.20 & 0.07 \\ 
$\sigma$ & 0.37 & 0.006 & & &  & 1 & 0.39 & -0.37 \\ 
$\phi$ & 0.31 & 0.039 & &  &  &  & 1 & 0.47 \\ 
$\tau$ & 0.23 & 0.005 & & &  & &  & 1 \\ 
\hline
\end{tabular}
\caption{Estimates are under joint 1997 and 2000 model}
\end{table}

\begin{itemize}
\item Likelihood ratio test performed for joint versus separate models:
\begin{itemize}
\item $p=0.007$ under 3 degrees of freedom
\end{itemize}
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{1997 Predictions}
\begin{center}
\image{width=1.2in}{pref5.png}
\image{width=1.2in}{pref50.png}
\image{width=1.45in}{pref95.png}\\
\vspace{-.05in} {\tiny Preferential (MLEs from \citep{diggle2010}): ($\h{\mu}_{97} = 1.515, \h{\mu}_{00} = 0.762, \h{\sigma}^2=0.138, \h{\phi}=0.313, \h{\tau}^2=0.059$)}
\\
\image{width=1.2in}{np5.png}
\image{width=1.2in}{np50.png}
\image{width=1.45in}{np95.png}\\
\vspace{-.05in} {\tiny Non-preferential (MLEs fit with naive initial guess): ($\h{\mu}_{97} = 1.551, \h{\mu}_{00} = 0.727, \h{\sigma}^2=0.136, \h{\phi}=0.305, \h{\tau}^2=0.052$)}
\end{center}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{1997 Predictions}
\begin{center}
\image{width=1.3in}{over3.pdf}
\image{width=1.3in}{over5.pdf}
\image{width=1.34in}{over7.pdf}
\\
{Areal proportion predicted over 3 (left), 5 (middle) and 7 (right) \\ $\mu$ g/(g dry weight) for preferential (solid) and non-preferential (dashed) MLEs}
\end{center}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Problems with the Methodology}
\begin{itemize}
\setlength\itemsep{.15in}
\item Using the joint model for 1997 predictions
\item Assuming independence of 1997 and 2000 data in fitting
\item No cross-validation performed
\item No evaluation of preferential model in simulation study
\item True preferential predictive distribution not used nor given
\begin{itemize}
\item True predictive distribution may be Gaussian, although its parameters are unclear
\end{itemize}
\item Non-preferential MLEs similar to preferential MLEs:
\begin{itemize}
\item My non-preferential model: \\
$(\h{\mu}_{97} = 1.551, \h{\mu}_{00} = 0.727, \h{\sigma}^2=0.136, \h{\phi}=0.305, \h{\tau}^2=0.052)$
\item Their preferential model: \\
$(\h{\mu}_{97} = 1.515, \h{\mu}_{00} = 0.762, \h{\sigma}^2=0.138, \h{\phi}=0.313, \h{\tau}^2=0.059)$
\end{itemize}
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Conclusions}
\begin{itemize}
\setlength\itemsep{.15in}
\item Accounting for preferentiality is important
\item For empirical variogram estimation and classical prediction:
\begin{itemize}
\item Uniform sampling performed best, then clustered, then preferential
\item For preferential data, naive variogram estimates and predictions were biased
\end{itemize}
\item Proposed class of models is flexible and tractable for geostatistics with preferential data
\end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------------------------------------------------------
%  Section 6
%----------------------------------------------------------------------------------------------------------------------------------------
\section{References}
\begin{frame}
\tiny
\frametitle{References}
\bibliographystyle{../te}
\bibliography{../prelim}
\end{frame}

%----------------------------------------------------------------------------------------------------------------------------------------
\begin{frame}
\frametitle{Appendix: Understanding the Preferential Likelihood}
 \bal
L(\theta) &= \int_S [\v{X} \vert \v{S}] [\v{Y} \vert \v{S}, \v{X}] \frac{[\v{S} \vert \v{X}, \v{Y}]_{np}}{[\v{S} \vert \v{X}, \v{Y}]_{np}} [\v{S}] \ d\v{S} \\
&= \int_S [\v{X} \vert \v{S}] [\v{Y} \vert \v{S}, \v{X}] \frac{[\v{S} \vert \v{X}, \v{Y}]_{np}}{[\v{S}_1 \vert \v{S}_0, \v{X}, \v{Y}]_{np} [\v{S}_0 \vert \v{X}, \v{Y}]_{np}} [\v{S}_1 \vert \v{S}_0] [\v{S}_0] \ d\v{S} \\
&= \int_S [\v{X} \vert \v{S}] \frac{[\v{Y} \vert \v{S}, \v{X}]}{[\v{S}_1 \vert \v{S}_0, \v{X}, \v{Y}]_{np} [\v{S}_0 \vert \v{X}, \v{Y}]_{np}} [\v{S}_1 \vert \v{S}_0] [\v{S}_0] [\v{S} \vert \v{X}, \v{Y}]_{np} \ d\v{S} \\
&= \int_S [\v{X} \vert \v{S}] \frac{[\v{Y} \vert \v{S}_0, \v{X}]}{[\v{S}_1 \vert \v{S}_0, \v{X}]_{np} [\v{S}_0 \vert \v{X}, \v{Y}]_{np}} [\v{S}_1 \vert \v{S}_0] [\v{S}_0] [\v{S} \vert \v{X}, \v{Y}]_{np} \ d\v{S} \\
&= \int_S [\v{X} \vert \v{S}] \frac{[\v{Y} \vert \v{S}_0, \v{X}]}{[\v{S}_1 \vert \v{S}_0, \v{X}]_{np} [\v{S}_0 \vert \v{X}, \v{Y}]_{np}} [\v{S}_1 \vert \v{S}_0] [\v{S}_0] [\v{S}  \vert \v{X} , \v{Y} ]_{np} \ d\v{S} \\
&= E_{[S \vert \v{X}, \v{Y}]_{np}} \brack{ [\v{X} \vert \v{S}] \frac{[\v{Y} \vert \v{S}_0, \v{X}]}{[\v{S}_0 \vert \v{X}, \v{Y}]_{np}} [\v{S}_0]  } 
\eal
\end{frame}
%---------------------------------------------------------------------------------------------------------------------------------------

\section{Appendix}

\end{document}








